BaseLine su 4000 training examples e 5 epochs:
"MSELoss_Adam_32_51_5": {"criterion": "MSELoss", "optimizer": "Adam", "batch_size": 32, "kernel_size": 51, "total_epoche": 5, "number_parameters": 21675452, "epoch_4": {"train_loss": [0.031600027810782194, 0.006817865858439112, 0.0061363944508488815, 0.005820924088182132, 0.005467598606920699], "train_psnr": [17.673595576068216, 21.745002476046132, 22.206872707825877, 22.442878904762715, 22.716684162712227], "valid_loss": [0.0073767360366348705, 0.006131714742098536, 0.005652454708303724, 0.006053704648677792, 0.0052064469283712765], "valid_psnr": [21.380704441138512, 22.20308808876787, 22.56214357038702, 22.242496039813684, 22.923732740526418]}, "test_results": {"test_loss": 0.005533515843887662, "test_psnr": 22.70703045642341}}}

con normalizzazione:
"MSELoss_Adam_32_51_5": {"criterion": "MSELoss", "optimizer": "Adam", "batch_size": 32, "kernel_size": 51, "total_epoche": 5, "number_parameters": 21675452, "epoch_4": {"train_loss": [2.3976576661038465, 0.007583398810404278, 0.006123791634016772, 0.005844786761491729, 0.005620700225614718], "train_psnr": [17.018957128031747, 21.301989221675463, 22.21857713148829, 22.42229941811709, 22.591149063025917], "valid_loss": [0.009233282280287572, 0.006186928899426546, 0.005716091953217983, 0.005466678113277469, 0.005375387892127037], "valid_psnr": [20.3922961335935, 22.168117805170578, 22.5218810995272, 22.722875573414616, 22.79227544531622]}, "test_results": {"test_loss": 0.005716169030990365, "test_psnr": 22.57258092148753}}}

normalizzazione e un layer convoluzionale in meno per ogni blocco basic:
{"MSELoss_Adam_32_51_5": {"criterion": "MSELoss", "optimizer": "Adam", "batch_size": 32, "kernel_size": 51, "total_epoche": 5, "number_parameters": 15397404, "epoch_4": {"train_loss": [0.008845675641494076, 0.005775656506935828, 0.005354488798736656, 0.005100666822348959, 0.004889414671223855], "train_psnr": [21.055154379027552, 22.472145260619758, 22.81156051529817, 23.02307801902994, 23.21074279364367], "valid_loss": [0.005961132734747869, 0.005550125081624303, 0.0050794013044131656, 0.004762713887196566, 0.0046227626635559965], "valid_psnr": [22.33370871522264, 22.646773778318618, 23.036452717999452, 23.327745050031133, 23.45829145301434]}, "test_results": {"test_loss": 0.004947268160406587, "test_psnr": 23.21302537079341}}}
ci mette meno tempo e ha una curva di learning monotona (probabilmente ha meno overfitting)

il dropout non introduce cambiamenti significativi

come prima ma con un upscaling bicubico:
"MSELoss_Adam_32_51_5": {"criterion": "MSELoss", "optimizer": "Adam", "batch_size": 32, "kernel_size": 51, "total_epoche": 5, "number_parameters": 15397404, "epoch_4": {"train_loss": [0.008858914947626691, 0.005698038710376424, 0.0052499359126889356, 0.005039016753571095, 0.004841757289059188], "train_psnr": [21.04825677214837, 22.537926361536933, 22.89484303196432, 23.075977811747133, 23.25507172950744], "valid_loss": [0.005831208771892957, 0.0051807618101260495, 0.004891381964885763, 0.004774454475513526, 0.004624177402417574], "valid_psnr": [22.43174141417205, 22.9517512030112, 23.206291150025233, 23.307598164969775, 23.451314133004477]}, "test_results": {"test_loss": 0.004948566586515585, "test_psnr": 23.205351779038004}}
impiega un po' di tempo in pi√π