For the creation of the dataset some things were modified:
- Patches are taken from images and videos
- to detect motion the histrogram is used between couples of images
- the parametrization is done by hand
- more than one patch is taken from an image
- frames are taken every n seconds to increase diversity
- videos are chosen with 480p resolution, so the motion in a patch is not too large
  also are chosen to have focus on all moving things, to not get blur and the motion is not too large
- Images are randomly cropped from videos, so they are not re cropped from 150 to 128 pixels
- Modificate le funzioni train e valutazione per rendere il codice più leggibile (si spera)
- Rimossa la metrica di accuracy e aggiunta quella di PSNR
- Aggiunta la possibilità di avere il numero totale di parametri
- aggiunta normalizzazione e successiva denormalizzazione in output per migliorare il learning (ciò in effetti 
  migliora un pochino la loss). Una volta calcolata la normalizzazione questa viene salvata su file 
  per evitare di calcolarla ogni volta
- usando l'output del layer conv4_4 di vgg16 come loss function, le immagini previste sono molto chiare, a discapito,
  nel caso ci sia un grosso movimento, della precisione dell'interpolazione. In questo caso viene dato in output 
  il frame 0 o 2
- Adam migliore di Adamax
- Il dropout non porta significativi miglioramenti
- Aggiunta la possibilità di distanziare gli elementi delle triplette nel dataset
- Aggiunta custom fixed kernel per la loss function, essa evidenzia i contorni e il colore
- Aggiunti 2 layer convoluzionali in uscita dell'immagine, per cercare di ridurre la sfocatura
- Facendo training sul logo del dvd, la rete riesce ad imparare a traslarlo correttamente (dopo 10 epoch), se dopo questo learning
  passo al dataset UCF-101 la loss iniziale è già piuttosto bassa (0.18 con fixed kernel)